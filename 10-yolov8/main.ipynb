{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f080e00",
   "metadata": {},
   "source": [
    "# 样例介绍\n",
    "* YOLOv5是一种单阶段目标检测算法，在这个样例中，我们选取了YOLOv5s，它是YOLOv5系列中较为轻量的网络，适合在边缘设备部署，进行实时目标检测。\n",
    "\n",
    "# 前期准备\n",
    "* 基础镜像的样例目录中已包含转换后的om模型以及测试图片，如果直接运行，可跳过此步骤。如果需要重新转换模型，可以参考下面的步骤。\n",
    "* 首先我们可以在[这个链接](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Atlas%20200I%20DK%20A2/DevKit/downloads/23.0.RC1/Ascend-devkit_23.0.RC1_downloads.xlsx)的表格中找到本样例的依赖文件，下载我们已经准备好了的ONNX模型，ONNX是开源的离线推理模型框架。\n",
    "\n",
    "* 为了能进一步优化模型推理性能，我们需要将其转换为om模型进行使用，以下为转换指令：  \n",
    "    ```shell\n",
    "    atc --model=yolov5s.onnx --framework=5 --output=yolo --input_format=NCHW --input_shape=\"input_image:1,3,640,640\" --log=error --soc_version=Ascend310B1\n",
    "    ```\n",
    "    * 其中转换参数的含义为：  \n",
    "        * --model：输入模型路径\n",
    "        * --framework：原始网络模型框架类型，5表示ONNX\n",
    "        * --output：输出模型路径\n",
    "        * --input_format：输入Tensor的内存排列方式\n",
    "        * --input_shape：指定模型输入数据的shape\n",
    "        * --log：日志级别\n",
    "        * --soc_version：昇腾AI处理器型号\n",
    "        * --input_fp16_nodes：指定输入数据类型为FP16的输入节点名称\n",
    "        * --output_type：指定网络输出数据类型或指定某个输出节点的输出类型\n",
    "\n",
    "# 模型推理实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d7761be-840e-4fc8-b501-7a5f9a0a56b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入代码依赖\n",
    "import cv2\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import torch\n",
    "from skvideo.io import vreader, FFmpegWriter\n",
    "import IPython.display\n",
    "from ais_bench.infer.interface import InferSession\n",
    "\n",
    "from det_utils import letterbox, scale_coords, nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e9292fc-f49c-410d-8bf3-08069171c2e0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image, cfg, bgr2rgb=True):\n",
    "    \"\"\"图片预处理\"\"\"\n",
    "    img, scale_ratio, pad_size = letterbox(image, new_shape=cfg['input_shape'])\n",
    "    if bgr2rgb:\n",
    "        img = img[:, :, ::-1]\n",
    "    img = img.transpose(2, 0, 1)  # HWC2CHW\n",
    "    img = np.ascontiguousarray(img, dtype=np.float32)\n",
    "    return img, scale_ratio, pad_size\n",
    "\n",
    "\n",
    "def draw_bbox(bbox, img0, color, wt, names):\n",
    "    \"\"\"在图片上画预测框\"\"\"\n",
    "    det_result_str = ''\n",
    "    for idx, class_id in enumerate(bbox[:, 5]):\n",
    "        if float(bbox[idx][4] < float(0.05)):\n",
    "            continue\n",
    "        img0 = cv2.rectangle(img0, (int(bbox[idx][0]), int(bbox[idx][1])), (int(bbox[idx][2]), int(bbox[idx][3])),\n",
    "                             color, wt)\n",
    "        img0 = cv2.putText(img0, str(idx) + ' ' + names[int(class_id)], (int(bbox[idx][0]), int(bbox[idx][1] + 16)),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "        img0 = cv2.putText(img0, '{:.4f}'.format(bbox[idx][4]), (int(bbox[idx][0]), int(bbox[idx][1] + 32)),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "        det_result_str += '{} {} {} {} {} {}\\n'.format(\n",
    "            names[bbox[idx][5]], str(bbox[idx][4]), bbox[idx][0], bbox[idx][1], bbox[idx][2], bbox[idx][3])\n",
    "    return img0\n",
    "\n",
    "\n",
    "def get_labels_from_txt(path):\n",
    "    \"\"\"从txt文件获取图片标签\"\"\"\n",
    "    labels_dict = dict()\n",
    "    with open(path) as f:\n",
    "        for cat_id, label in enumerate(f.readlines()):\n",
    "            labels_dict[cat_id] = label.strip()\n",
    "    return labels_dict\n",
    "\n",
    "\n",
    "def draw_prediction(pred, image, labels):\n",
    "    \"\"\"在图片上画出预测框并进行可视化展示\"\"\"\n",
    "    imgbox = widgets.Image(format='jpg', height=720, width=1280)\n",
    "    img_dw = draw_bbox(pred, image, (0, 255, 0), 2, labels)\n",
    "    imgbox.value = cv2.imencode('.jpg', img_dw)[1].tobytes()\n",
    "    display(imgbox)\n",
    "\n",
    "\n",
    "def infer_image(img_path, model, class_names, cfg):\n",
    "    \"\"\"图片推理\"\"\"\n",
    "    # 图片载入\n",
    "    image = cv2.imread(img_path)\n",
    "    # 数据预处理\n",
    "    img, scale_ratio, pad_size = preprocess_image(image, cfg)\n",
    "    # 模型推理\n",
    "    output = model.infer([img])[0]\n",
    "\n",
    "    output = torch.tensor(output)\n",
    "    # 非极大值抑制后处理\n",
    "    boxout = nms(output, conf_thres=cfg[\"conf_thres\"], iou_thres=cfg[\"iou_thres\"])\n",
    "    pred_all = boxout[0].numpy()\n",
    "    # 预测坐标转换\n",
    "    scale_coords(cfg['input_shape'], pred_all[:, :4], image.shape, ratio_pad=(scale_ratio, pad_size))\n",
    "    # 图片预测结果可视化\n",
    "    draw_prediction(pred_all, image, class_names)\n",
    "\n",
    "\n",
    "def infer_frame_with_vis(image, model, labels_dict, cfg, bgr2rgb=True):\n",
    "    # 数据预处理\n",
    "    img, scale_ratio, pad_size = preprocess_image(image, cfg, bgr2rgb)\n",
    "    # 模型推理\n",
    "    output = model.infer([img])[0]\n",
    "\n",
    "    output = torch.tensor(output)\n",
    "    # 非极大值抑制后处理\n",
    "    boxout = nms(output, conf_thres=cfg[\"conf_thres\"], iou_thres=cfg[\"iou_thres\"])\n",
    "    pred_all = boxout[0].numpy()\n",
    "    # 预测坐标转换\n",
    "    scale_coords(cfg['input_shape'], pred_all[:, :4], image.shape, ratio_pad=(scale_ratio, pad_size))\n",
    "    # 图片预测结果可视化\n",
    "    img_vis = draw_bbox(pred_all, image, (0, 255, 0), 2, labels_dict)\n",
    "    return img_vis\n",
    "\n",
    "\n",
    "def img2bytes(image):\n",
    "    \"\"\"将图片转换为字节码\"\"\"\n",
    "    return bytes(cv2.imencode('.jpg', image)[1])\n",
    "\n",
    "\n",
    "def infer_video(video_path, model, labels_dict, cfg):\n",
    "    \"\"\"视频推理\"\"\"\n",
    "    image_widget = widgets.Image(format='jpeg', width=800, height=600)\n",
    "    display(image_widget)\n",
    "\n",
    "    # 读入视频\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while True:\n",
    "        ret, img_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # 对视频帧进行推理\n",
    "        image_pred = infer_frame_with_vis(img_frame, model, labels_dict, cfg, bgr2rgb=True)\n",
    "        image_widget.value = img2bytes(image_pred)\n",
    "\n",
    "\n",
    "def infer_camera(model, labels_dict, cfg):\n",
    "    \"\"\"外设摄像头实时推理\"\"\"\n",
    "    def find_camera_index():\n",
    "        max_index_to_check = 10  # Maximum index to check for camera\n",
    "\n",
    "        for index in range(max_index_to_check):\n",
    "            cap = cv2.VideoCapture(index)\n",
    "            if cap.read()[0]:\n",
    "                cap.release()\n",
    "                return index\n",
    "\n",
    "        # If no camera is found\n",
    "        raise ValueError(\"No camera found.\")\n",
    "\n",
    "    # 获取摄像头\n",
    "    camera_index = find_camera_index()\n",
    "    cap = cv2.VideoCapture(camera_index)\n",
    "    # 初始化可视化对象\n",
    "    image_widget = widgets.Image(format='jpeg', width=1280, height=720)\n",
    "    display(image_widget)\n",
    "    while True:\n",
    "        # 对摄像头每一帧进行推理和可视化\n",
    "        _, img_frame = cap.read()\n",
    "        image_pred = infer_frame_with_vis(img_frame, model, labels_dict, cfg)\n",
    "        image_widget.value = img2bytes(image_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3671054-f2d0-4ff4-829f-e112200a65f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c2b7183-f449-48eb-a36f-ca150cea3a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前处理\n",
    "def resize_image(image, size, letterbox_image):\n",
    "    \"\"\"\n",
    "        对输入图像进行resize\n",
    "    Args:\n",
    "        size:目标尺寸\n",
    "        letterbox_image: bool 是否进行letterbox变换\n",
    "    Returns:指定尺寸的图像\n",
    "    \"\"\"\n",
    "    ih, iw, _ = image.shape\n",
    "    print(ih, iw)\n",
    "    h, w = size\n",
    "    # letterbox_image = False\n",
    "    if letterbox_image:\n",
    "        scale = min(w/iw, h/ih)\n",
    "        nw = int(iw*scale)\n",
    "        nh = int(ih*scale)\n",
    "        image = cv2.resize(image, (nw, nh), interpolation=cv2.INTER_LINEAR)\n",
    "        # cv2.imshow(\"img\", img)\n",
    "        # cv2.waitKey()\n",
    "        # print(image.shape)\n",
    "        # 生成画布\n",
    "        image_back = np.ones((h, w, 3), dtype=np.uint8) * 128\n",
    "        # 将image放在画布中心区域-letterbox\n",
    "        image_back[(h-nh)//2: (h-nh)//2 + nh, (w-nw)//2:(w-nw)//2+nw , :] = image\n",
    "    else:\n",
    "        image_back = image\n",
    "        # cv2.imshow(\"img\", image_back)\n",
    "        # cv2.waitKey()\n",
    "    return image_back  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57847e9c-89d1-4f92-97f6-4afba269ca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2input(img):\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    img = img/255\n",
    "    return np.expand_dims(img, axis=0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0417d90-4eaf-4733-ade3-687caf2b23f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh2xyxy(*box):\n",
    "    \"\"\"\n",
    "    将xywh转换为左上角点和左下角点\n",
    "    Args:\n",
    "        box:\n",
    "    Returns: x1y1x2y2\n",
    "    \"\"\"\n",
    "    ret = [box[0] - box[2] // 2, box[1] - box[3] // 2, \\\n",
    "          box[0] + box[2] // 2, box[1] + box[3] // 2]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45d1972a-819f-410d-ba8b-28434b43ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cod_trf(result, pre, after):\n",
    "    \"\"\"\n",
    "    因为预测框是在经过letterbox后的图像上做预测所以需要将预测框的坐标映射回原图像上\n",
    "    Args:\n",
    "        result:  [x,y,w,h,conf(最大类别概率), x1,y1,conf1, x2,y2,conf2, ..., x21,y21,conf21]\n",
    "        pre:    原尺寸图像\n",
    "        after:  经过letterbox处理后的图像\n",
    "    Returns: 坐标变换后的结果,\n",
    "    \"\"\"\n",
    "    res = np.array(result)\n",
    "    # 提取预测框的坐标和置信度\n",
    "    x, y, w, h, conf = res[:5]\n",
    "    # 提取关键点的坐标和置信度\n",
    "    keypoints = res[5:].reshape(-1, 3)\n",
    "    \n",
    "    # 将[x, y, w, h]转换为[x1, y1, x2, y2]\n",
    "    x1, y1, x2, y2 = xywh2xyxy(x, y, w, h)\n",
    "    \n",
    "    # 获取原图像和经过letterbox处理后的图像的尺寸\n",
    "    h_pre, w_pre, _ = pre.shape\n",
    "    h_after, w_after, _ = after.shape\n",
    "    \n",
    "    # 计算缩放比例和平移量\n",
    "    scale = max(w_pre/w_after, h_pre/h_after)\n",
    "    h_pre, w_pre = h_pre/scale, w_pre/scale\n",
    "    x_move, y_move = abs(w_pre-w_after)//2, abs(h_pre-h_after)//2\n",
    "    \n",
    "    # 变换预测框的坐标\n",
    "    ret_x1, ret_x2 = (x1 - x_move) * scale, (x2 - x_move) * scale\n",
    "    ret_y1, ret_y2 = (y1 - y_move) * scale, (y2 - y_move) * scale\n",
    "    ret_box = [int(ret_x1), int(ret_y1), int(ret_x2), int(ret_y2), conf]\n",
    "    \n",
    "    # 变换关键点的坐标\n",
    "    ret_keypoints = []\n",
    "    for kp in keypoints:\n",
    "        kp_x, kp_y, kp_conf = kp\n",
    "        ret_kp_x = (kp_x - x_move) * scale\n",
    "        ret_kp_y = (kp_y - y_move) * scale\n",
    "        ret_keypoints.extend([int(ret_kp_x), int(ret_kp_y), kp_conf])\n",
    "    \n",
    "    # 组合预测框和关键点的结果\n",
    "    ret = np.array(ret_box + ret_keypoints)\n",
    "    return ret\n",
    "\n",
    "# 假设 pre 和 after 是已经定义的图像\n",
    "# highest_conf_pred = np.array(...)  # 您的68维numpy数组\n",
    "# transformed_result = cod_trf(highest_conf_pred, pre, after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2b6fd4a-ed48-4149-805c-9c01a9f115d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(res, image, color):\n",
    "    \"\"\"\n",
    "    将预测框和关键点绘制在image上\n",
    "    Args:\n",
    "        res: 预测框数据\n",
    "        image: 原图\n",
    "    Returns:\n",
    "        image: 绘制了预测框和关键点的图像\n",
    "    \"\"\"\n",
    "    # 假设 res 的结构是 [x1, y1, x2, y2, conf, x1_kp, y1_kp, conf1_kp, ..., x21_kp, y21_kp, conf21_kp]\n",
    "    # 其中前五个元素是预测框的坐标和置信度，后面是21个关键点的坐标和置信度\n",
    "    \n",
    "    # 提取预测框的坐标和置信度\n",
    "    x1, y1, x2, y2, conf = res[:5]\n",
    "    \n",
    "    # 画框 后两个参数：颜色和线宽\n",
    "    image = cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 5)\n",
    "    \n",
    "    # 计算预测框的长宽\n",
    "    h, w = int(y2) - int(y1), int(x2) - int(x1)\n",
    "    \n",
    "    # 计算字体大小（随框大小调整）\n",
    "    font_size = min(h/640, w/640) * 3\n",
    "    \n",
    "    # 确保字体大小不小于3\n",
    "    font_size = max(font_size, 3)\n",
    "    \n",
    "    # 绘制文本（这里我们使用分数作为文本，因为没有类别名称）\n",
    "    text = \"{:.2f}\".format(float(conf))\n",
    "    \n",
    "    # 绘制文本\n",
    "    image = cv2.putText(image, text, (max(10, int(x1)), max(10, int(y1) - 5)), cv2.FONT_HERSHEY_COMPLEX, font_size, (0, 255, 0), 10)  # 绿色文本\n",
    "    \n",
    "    # 提取关键点的坐标和置信度\n",
    "    keypoints = res[5:].reshape(-1, 3)\n",
    "    \n",
    "    # 绘制关键点\n",
    "    for i, kp in enumerate(keypoints):\n",
    "        kp_x, kp_y, kp_conf = kp\n",
    "        # color\n",
    "        kpt_color = color[i]['color']\n",
    "        # 画关键点：图片、XY坐标、半径、颜色、线宽（-1为填充）\n",
    "        image = cv2.circle(image, (int(kp_x), int(kp_y)), 20, kpt_color, -1)\n",
    "        \n",
    "        # 标注关键点编号 字体大小 字体颜色 线宽\n",
    "        image = cv2.putText(image, str(i), (int(kp_x) + 5, int(kp_y) + 5), cv2.FONT_HERSHEY_SIMPLEX, 5, kpt_color, 5)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# 假设 bbox_res 是经过 cod_trf 函数处理后的数据\n",
    "# img = cv2.imread('DSC_5384.jpg')\n",
    "# drawn_image = draw(bbox_res, img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e455e8d",
   "metadata": {},
   "source": [
    "# 样例运行\n",
    "\n",
    "* 初始化相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dccded13-fad1-4b88-b63f-d46deb23aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'conf_thres': 0.4,  # 模型置信度阈值，阈值越低，得到的预测框越多\n",
    "    'iou_thres': 0.5,  # IOU阈值，高于这个阈值的重叠预测框会被过滤掉\n",
    "    'input_shape': [640, 640],  # 模型输入尺寸\n",
    "}\n",
    "\n",
    "model_path = 'yolov8n.om'\n",
    "label_path = './coco_names.txt'\n",
    "# 初始化推理模型\n",
    "model = InferSession(0, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb8b1ae0-3ba9-47ee-adc3-cb0ba9392f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('fch_ear.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00ffc401-bc0e-4ace-8ee7-46f422c15a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_h, std_w = 640, 640  # 标准输入尺寸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "202fac40-af09-4d21-a7c2-06bd827feff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3557 2836\n"
     ]
    }
   ],
   "source": [
    "# 前处理\n",
    "img_after = resize_image(img, (std_w, std_h), True)  # （640， 640， 3）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b556139-0202-45d0-a89e-1fe18c2f3c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 640, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_after.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b38c4967-0213-4b98-a972-bcda1d9adeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将图像处理成输入的格式\n",
    "data = img2input(img_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af2e4e89-4e30-42f1-85d5-b0541612d3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 640, 640)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08ffcccd-dc80-4992-bea2-b43653eebc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.infer([data])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d1b6c7a-ac67-45fc-8bad-319aa32a4fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 68, 8400)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4952a5a7-9a8f-4c8a-a9cb-094585407135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8400, 68)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将模型输出调整为 (8400, 68) 形状，以便于处理\n",
    "pred_reshaped = output.squeeze().transpose((1, 0))\n",
    "pred_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "135f33da-a5d6-42cc-9247-d15eff2a7160",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidences = pred_reshaped[:, 4]\n",
    "max_conf_index = np.argmax(confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d15ab65-3ba1-4417-bbd8-ece7a9c302d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8262"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_conf_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5056a424-8a61-464c-b66d-af1e6affa417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_conf_pred = pred_reshaped[max_conf_index]\n",
    "highest_conf_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5ffdea9-d7fc-4d33-89bb-084ba7364001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.6750000e+01,  4.5600000e+02,  1.3550000e+02,  3.2700000e+02,\n",
       "        2.2621155e-03, -3.9375000e+00,  4.2425000e+02,  3.6224365e-02,\n",
       "        5.8437500e+00,  4.2825000e+02,  4.9755859e-01,  1.9968750e+01,\n",
       "        4.5075000e+02,  3.7414551e-02,  1.7593750e+01,  4.2625000e+02,\n",
       "        5.2825928e-02,  7.0625000e+00,  4.3450000e+02,  3.1799316e-02,\n",
       "       -6.2500000e-01,  4.4375000e+02,  5.6762695e-02,  1.0343750e+01,\n",
       "        4.3150000e+02,  2.0248413e-02,  1.6437500e+01,  4.3775000e+02,\n",
       "        1.8615723e-02, -4.6875000e+00,  4.5450000e+02,  2.5939941e-02,\n",
       "       -1.9375000e+00,  4.3125000e+02,  2.1789551e-02,  4.9062500e+00,\n",
       "        4.5825000e+02,  7.2265625e-02,  1.5625000e-01,  4.6475000e+02,\n",
       "        4.7851562e-02,  1.3343750e+01,  4.5250000e+02,  1.5234375e-01,\n",
       "        1.4062500e+01,  4.6025000e+02,  1.4257812e-01,  3.8437500e+00,\n",
       "        4.4250000e+02,  5.0292969e-02,  5.6250000e+00,  4.4050000e+02,\n",
       "        1.1010742e-01,  2.1031250e+01,  4.1775000e+02,  7.5317383e-02,\n",
       "        2.0250000e+01,  4.5000000e+02,  6.9824219e-02,  2.3062500e+01,\n",
       "        4.3075000e+02,  7.1228027e-02,  1.9281250e+01,  4.4400000e+02,\n",
       "        4.9438477e-02, -2.5000000e-01,  4.3575000e+02,  9.0087891e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_conf_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d54db2c-49f8-404d-8a9b-70b85050a70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_res = cod_trf(highest_conf_pred, img, img_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9a57a1c-33cb-4699-a280-b1213e02a219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.57000000e+02,  1.62800000e+03,  3.87000000e+02,  3.44000000e+03,\n",
       "        2.26211548e-03, -3.77000000e+02,  2.35700000e+03,  3.62243652e-02,\n",
       "       -3.23000000e+02,  2.38000000e+03,  4.97558594e-01, -2.44000000e+02,\n",
       "        2.50500000e+03,  3.74145508e-02, -2.57000000e+02,  2.36900000e+03,\n",
       "        5.28259277e-02, -3.16000000e+02,  2.41400000e+03,  3.17993164e-02,\n",
       "       -3.59000000e+02,  2.46600000e+03,  5.67626953e-02, -2.98000000e+02,\n",
       "        2.39800000e+03,  2.02484131e-02, -2.64000000e+02,  2.43200000e+03,\n",
       "        1.86157227e-02, -3.81000000e+02,  2.52600000e+03,  2.59399414e-02,\n",
       "       -3.66000000e+02,  2.39600000e+03,  2.17895508e-02, -3.28000000e+02,\n",
       "        2.54600000e+03,  7.22656250e-02, -3.54000000e+02,  2.58200000e+03,\n",
       "        4.78515625e-02, -2.81000000e+02,  2.51400000e+03,  1.52343750e-01,\n",
       "       -2.77000000e+02,  2.55700000e+03,  1.42578125e-01, -3.34000000e+02,\n",
       "        2.45900000e+03,  5.02929688e-02, -3.24000000e+02,  2.44800000e+03,\n",
       "        1.10107422e-01, -2.38000000e+02,  2.32100000e+03,  7.53173828e-02,\n",
       "       -2.43000000e+02,  2.50100000e+03,  6.98242188e-02, -2.27000000e+02,\n",
       "        2.39400000e+03,  7.12280273e-02, -2.48000000e+02,  2.46700000e+03,\n",
       "        4.94384766e-02, -3.57000000e+02,  2.42100000e+03,  9.00878906e-02])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd0ad756-027c-48f0-965d-de8fab5d4675",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoint_info = {\n",
    "    0: {'name': '肾上腺', 'id': 0, 'color': [101, 205, 228]},\n",
    "    1: {'name': '耳尖', 'id': 1, 'color': [240, 128, 128]},\n",
    "    2: {'name': '胃', 'id': 2, 'color': [154, 205, 50]},\n",
    "    3: {'name': '眼', 'id': 3, 'color': [34, 139, 34]},\n",
    "    4: {'name': '口', 'id': 4, 'color': [139, 0, 0]},\n",
    "    5: {'name': '肝', 'id': 5, 'color': [255, 165, 0]},\n",
    "    6: {'name': '对屏尖', 'id': 6, 'color': [255, 0, 255]},\n",
    "    7: {'name': '心', 'id': 7, 'color': [255, 255, 0]},\n",
    "    8: {'name': '肺', 'id': 8, 'color': [29, 123,243]},\n",
    "    9: {'name': '肺2', 'id': 9, 'color': [0, 255, 255]},\n",
    "    10: {'name': '膀胱', 'id': 10, 'color': [128, 0, 128]},\n",
    "    11: {'name': '脾', 'id': 11, 'color': [74, 181, 57]},\n",
    "    12: {'name': '角窝中', 'id': 12, 'color': [165, 42, 42]},\n",
    "    13: {'name': '神门', 'id': 13, 'color': [128, 128, 0]},\n",
    "    14: {'name': '肾', 'id': 14, 'color': [255, 0, 0]},\n",
    "    15: {'name': '耳门', 'id': 15, 'color': [34, 139, 34]},\n",
    "    16: {'name': '听宫', 'id': 16, 'color': [255, 129, 0]},\n",
    "    17: {'name': '听会', 'id': 17, 'color': [70, 130, 180]},\n",
    "    18: {'name': '肩', 'id': 18, 'color': [63, 103,165]},\n",
    "    19: {'name': '扁桃体', 'id': 19, 'color': [66, 77, 229]},\n",
    "    20: {'name': '腰骶椎', 'id': 20, 'color': [255, 105, 180]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a8ca47d5-6245-4eb6-a1cd-8fd48f04628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = draw(bbox_res, img, keypoint_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1d6f02d4-bba2-468c-a05f-feedc4eeeaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"output.jpg\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766dd627-ee08-4984-a90f-24005d978708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1916297e-91fd-431c-b89c-dcf70763159b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608553b-484c-4bb5-9ea1-e9cf80cda5eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356c525b-e44a-4a54-94bf-499a5039e501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ca90e901-e156-4002-b542-4ee97bf76374",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) /io/opencv/modules/highgui/src/window.cpp:1272: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19287/154020694.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"result\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.9.0) /io/opencv/modules/highgui/src/window.cpp:1272: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n"
     ]
    }
   ],
   "source": [
    "cv2.namedWindow(\"result\", cv2.WINDOW_NORMAL)\n",
    "cv2.imshow(\"result\", image)\n",
    "key = cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3342d16c-fd6e-46bc-ac8e-eef30075f0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89e1f181-b5a9-4383-b551-b043c0686e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.17992690469496767, 0.17992690469496767)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b656f44-d271-4faf-a239-3ae31e02fa80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65.0, 0.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bb4cf96-2aad-41d2-82ea-2fa5ffe00a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型推理\n",
    "output = model.infer([img])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "269e6002-1878-4a9e-a4b1-baa92a67a3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 68, 8400)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01102e04-5e24-41fe-a377-f580a7ede8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8400, 68)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将模型输出调整为 (8400, 68) 形状，以便于处理\n",
    "pred_reshaped = output.squeeze().transpose((1, 0))\n",
    "pred_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e36d115b-2094-4f3e-80bb-b1cb3b1e887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidences = pred_reshaped[:, 4]\n",
    "max_conf_index = np.argmax(confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "356f9025-12cf-40ff-8be0-57107ba06ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5258789e-05"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23041677-edfe-4244-9bc6-721459a13b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6240"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_conf_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa1951ad-868f-473d-9735-ddac7b0afbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_conf_pred = pred_reshaped[max_conf_index]\n",
    "highest_conf_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a894c74-dc30-45d1-bf6f-45667a6fc8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.48437500e+00,  6.35000000e+02,  9.90625000e+00,  1.24500000e+02,\n",
       "        1.00000000e+00, -1.37890625e+01,  6.36500000e+02,  8.37707520e-03,\n",
       "        2.03125000e+01,  6.27500000e+02,  2.42187500e-01,  9.59375000e+00,\n",
       "        6.33500000e+02,  4.71496582e-03,  4.05517578e-01,  6.27500000e+02,\n",
       "        9.82666016e-03, -1.90156250e+01,  6.47000000e+02,  1.04904175e-02,\n",
       "       -4.14843750e+00,  6.30500000e+02,  1.29241943e-02, -1.06328125e+01,\n",
       "        6.37000000e+02,  1.10778809e-02, -1.53359375e+01,  6.36000000e+02,\n",
       "        1.01318359e-02, -1.11796875e+01,  6.22500000e+02,  4.55474854e-03,\n",
       "       -1.22343750e+01,  6.43500000e+02,  4.96292114e-03, -1.01875000e+01,\n",
       "        6.43500000e+02,  8.78143311e-03, -6.18164062e-01,  6.17000000e+02,\n",
       "        1.15051270e-02, -3.60546875e+00,  6.25500000e+02,  1.88903809e-02,\n",
       "        6.27734375e+00,  6.26000000e+02,  6.74438477e-03,  1.23828125e+00,\n",
       "        6.39500000e+02,  2.14080811e-02, -1.54140625e+01,  6.28000000e+02,\n",
       "        2.00195312e-02, -3.74062500e+01,  6.69000000e+02,  3.99169922e-02,\n",
       "       -2.54687500e+01,  6.52000000e+02,  3.33251953e-02,  1.74687500e+01,\n",
       "        6.51000000e+02,  2.52685547e-02, -1.12968750e+01,  6.39500000e+02,\n",
       "        7.03430176e-03,  4.98437500e+00,  6.33000000e+02,  1.01699829e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_conf_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85d7bdfe-e5e3-4907-b3dd-142784da71a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17244/4224307334.py\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mscale_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_shape'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio_pad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 图片预测结果可视化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdraw_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'class_names' is not defined"
     ]
    }
   ],
   "source": [
    "output = torch.tensor(output)\n",
    "# 非极大值抑制后处理\n",
    "boxout = nms(output, conf_thres=cfg[\"conf_thres\"], iou_thres=cfg[\"iou_thres\"])\n",
    "pred_all = boxout[0].numpy()\n",
    "# 预测坐标转换\n",
    "scale_coords(cfg['input_shape'], pred_all[:, :4], image.shape, ratio_pad=(scale_ratio, pad_size))\n",
    "# 图片预测结果可视化\n",
    "draw_prediction(pred_all, image, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e9cf34",
   "metadata": {},
   "source": [
    "* 选择推理模式。\"infer_mode\"有三个取值：image, camera, video，分别对应图片推理、摄像头实时推理和视频推理。默认使用视频推理模式。\n",
    "* 我们选取的样例是一个赛车视频，执行下面的代码后可以看到模型会对视频的每一帧进行推理，并将预测结果展示在画面上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aab68b6b-7d65-4dd9-ab0b-c5c9733c415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] acl init success\n",
      "[INFO] open device 0 success\n",
      "[INFO] load model yolo.om success\n",
      "[INFO] create model description success\n",
      "[INFO] load model yolov8n.om success\n",
      "[INFO] create model description success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@63.830] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@63.832] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@63.834] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video1): can't open camera by index\n",
      "[ERROR:0@63.835] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@63.836] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video2): can't open camera by index\n",
      "[ERROR:0@63.837] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@63.837] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video3): can't open camera by index\n",
      "[ERROR:0@63.838] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@63.839] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video4): can't open camera by index\n",
      "[ERROR:0@63.839] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@63.840] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video5): can't open camera by index\n",
      "[ERROR:0@63.841] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@63.841] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video6): can't open camera by index\n",
      "[ERROR:0@63.842] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@63.842] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video7): can't open camera by index\n",
      "[ERROR:0@63.843] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@63.843] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video8): can't open camera by index\n",
      "[ERROR:0@63.844] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@63.844] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video9): can't open camera by index\n",
      "[ERROR:0@63.844] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No camera found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16738/2101816269.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minfer_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0minfer_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'camera'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0minfer_camera\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0minfer_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'video'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvideo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'racing.mp4'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16738/2968311708.py\u001b[0m in \u001b[0;36minfer_camera\u001b[0;34m(model, labels_dict, cfg)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;31m# 获取摄像头\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mcamera_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_camera_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0mcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcamera_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;31m# 初始化可视化对象\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16738/2968311708.py\u001b[0m in \u001b[0;36mfind_camera_index\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# If no camera is found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No camera found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;31m# 获取摄像头\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No camera found."
     ]
    }
   ],
   "source": [
    "infer_mode = 'camera'\n",
    "\n",
    "if infer_mode == 'image':\n",
    "    img_path = 'world_cup.jpg'\n",
    "    infer_image(img_path, model, labels_dict, cfg)\n",
    "elif infer_mode == 'camera':\n",
    "    infer_camera(model, labels_dict, cfg)\n",
    "elif infer_mode == 'video':\n",
    "    video_path = 'racing.mp4'\n",
    "    infer_video(video_path, model, labels_dict, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27887b77",
   "metadata": {},
   "source": [
    "# 样例总结与扩展\n",
    "以上就是这个样例的全部内容了，值得关注的是在模型推理后有一步非常重要的后处理，就是非极大值抑制，即NMS，由于模型的原始预测结果会有非常多无效或重叠的预测框，我们需要通过NMS来进行过滤。再者，模型预测框的表示往往是一个标准化的结果，比如0到1之间，我们需要通过坐标转换将结果与原始图片的宽高对应上。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
